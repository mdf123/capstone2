---
title: "Capstone 2 Prediction of ALL vs AML from Gene Expression"
author: "Dr. Hans M. Rupp"
date: "29 7 2022"
output: pdf_document
---


# Introduction

The purpose of this project is to evaluate different machine learning algorithms for predicting two different cancer types, Acute Myeloid Leukemia (AML) and Acute lymphoblastic leukaemia (ALL) from microarray RNA expression data.
AML and ALL are the two main types of Acute Leukemias. They differ in their origin and treatments. To provide optimal treatment the cancer type must be diagnosed [1]. Microarray Expression chips are arrays of oligonucleotides which hybridize to mRNAs of specific genes. They allow the semiquantitative analysis of the expression a great number of genes in parallel. One common application is the comparision of diseased vs disease free or of different diseases to find differentially expressed (DE) genes [2]

I will use the AML ALL microarray dataset by Golub [3]. The csv files can be downloaded from
https://www.kaggle.com/datasets/crawford/gene-expression

```{r message=FALSE, warning=FALSE }
rm(list=ls())
if (!requireNamespace("BiocManager", quietly = TRUE))
  install.packages("BiocManager")
if (!require("limma"))
  install("limma")
if (!require("Biobase"))
  install("Biobase")

library(tidyverse)
library(caret)
library(matrixStats)
library(limma)
library(Biobase)


set.seed(2, sample.kind = "Rounding")
```


# Loading the data
```{r message=FALSE,warning=FALSE}
train_data <- read_csv("data_set_ALL_AML_train.csv", col_names = T)
test_data <- read_csv("data_set_ALL_AML_independent.csv")
labels <- read_csv("actual.csv")

```
# Checking and cleaning the data

Both train_data and test_data contain call columns which will not be used. We are only interested in gene expression data.
```{r}
head(train_data)
```
```{r}
train_data <- train_data %>% select(-contains("call"))
test_data <- test_data %>% select(-contains("call"))
```
Checking for missing values
```{r}
nas <- function(x) ( any(is.na(x)) )

print(train_data %>% summarise_all(nas), width = Inf)
print(test_data %>% summarise_all(nas), width = Inf)
```
There are no missing values.

We well merge the train_data and test_data set. Training and Testing data for ML alogorithms will be separated further down

```{r}
all_data <- train_data %>% left_join(test_data)
```

Inspecting the all_data set with View(all_data) shows that the data columns are not sorted according to the patient number.
Sorting:

```{r}
all_data_desc <- all_data[, 1:2]
all_data_data <- all_data[, 3:ncol(all_data)]
all_data_data <- all_data_data[, order(as.numeric(names(all_data_data)))]

all_data <- cbind(all_data_desc, all_data_data)
```

all_data now contains two columns with descriptions of the genes analyzed and `r ncol(all_data) -2` columns with gene expression data from the different patients. `r nrow(all_data)` genes are in rows.
labels contains  `r nrow(labels)` entries with patient number and cancer type.

We have `r nrow(all_data)` gene expression levels. It is highly unlikely that all or most of them are differentially regulated between these related tumors. Most will only contribute noise to any analysis. In a first step it is necessary to identify the differentially expressed (DE) genes.

# Selection of differentially  expressed genes

DE genes could be identified with a Student's T-test. However the package limma provides a more robust and convenient implementation for DE identification. [4] Limma is part of the Bioconductor project at http://www.bioconductor.org.

Construction of a Biobase ExpressionSet Object
```{r}
#features
f <- all_data[, 1:2]

#expression matrix
x <- as.matrix(all_data[, 3:ncol(all_data)])


#Colnames for expression set necessary
colnames(x) <- 1:ncol(x)

eset <- ExpressionSet(assayData = x,
                      phenoData = AnnotatedDataFrame(labels),
                      featureData = AnnotatedDataFrame(f))
```

Mean-reference model [5]

ALL and AML are the factors or classifiers associated with the samples in the experiment.

Y - Expression level of the gene

$\beta{_1}$ - Expression level for ALL

$\beta{_2}$ - Difference of expression between AML and ALL

Y = $\beta{_1}$ + $\beta{_2}$AML


Design Matrix

Design matrices are used in the estimation process of model parameters. The design matrix has columns associated with the parameters and rows associated with samples [5]
```{r}
design <- model.matrix(~cancer, data=pData(eset))
```
The number of rows in the design matrix is the number of observations
```{r}
nrow(design)
```
The number of AML observations is equal to the number of AML observations in the label set
```{r}
colSums(design)
sum(labels$cancer=="AML")
```

Estimate the fold changes and standard errors by tting a linear model for each gene

```{r}

fit <- lmFit(eset, design)
```
Apply empirical Bayes smoothing to the standard errors

```{r}
fit <- eBayes(fit)

results <- decideTests(fit[, "cancerAML"])
summary(results)
```

Top 100 differentially expressed genes
```{r}
top_DE_genes <- topTable(fit, n=100, adjust="fdr")
head(top_DE_genes)
```

Get the data of the top 100 DE genes
```{r}
top_data <- all_data %>% filter(`Gene Accession Number` %in% top_DE_genes$Gene.Accession.Number)
```


Comparison of the expression of the top DE gene
```{r, message=FALSE, fig.width=5, fig.height=3}
zyxin <- top_data %>% filter(`Gene Description`=="Zyxin") %>% select(-c(1,2)) 
zyxin <- as.numeric(t(zyxin))
zyxindf <- data.frame(x = zyxin, labels)
zyxindf %>% ggplot(aes(x=cancer, y=x)) + geom_boxplot() + labs(title = "Zyxin Expression", x="Cancer", y="Expression")
```





## Prepare Data for ML algorithms

Numerical Expression Matrix
```{r}
x_m <- top_data[, 3:ncol(top_data)]

```

Transpose to make tidy and add row and col names
```{r}
x_m <- t(x_m)
ncol(x_m) #100 top DE genes
nrow(x_m) ##72 patients

colnames(x_m) <- top_data$`Gene Accession Number`
#Paste cancer type to patient number
rownames(x_m)<- paste(rownames(x_m), labels$cancer)

```

## Heatmap

A heatmap is a visual representation of a multidimensional data set using a false-colored image.
By default the heatmap function clusters both features and samples. It uses the Eucledian distance and complete linkage.
The features are expected in rows.
The data is scaled

```{r}
heatmap(t(x_m))
```
We see that the AML cases are clustered on the left side.

## Normalization

The means of the expression data of different genes differ.
```{r, message=FALSE, fig.width=5, fig.height=3}
means <- data.frame(expm=rowMeans(x_m))
means %>% ggplot(aes(x=expm)) + geom_histogram()
```

To avoid that strongly expressed have a disproportionate influence on models we normalize the data

```{r}
x_m_norm <- scale(x_m)
```

Creation of train- and testdata

```{r}
indexTrain <- createDataPartition(labels$cancer, p=0.66, list = F)
norm_train_data <- x_m_norm[indexTrain, ]
norm_test_data <- x_m_norm[-indexTrain, ]
train_label <- labels[indexTrain, ]
test_label <- labels[-indexTrain, ]
```

# K-nearest-neighbours kNN
The kNN algorithm uses information about an example's k nearest neighbors to classify unlabeled examples. k is the only parameter of this algorithm. Determining the nearest neighbor requires a distance function. Usually the Euclidean distance is applied. [6]

The Euclidean distance between the samples p and q is:
dist(p,q) = $\sqrt{(p_{1}-q_{1})^2 + (p_{2}-q_{2})^2 + .. + (p_{n}-q_{n})^2}$
where p~1~ is the first features of sample p

Running knn with ks from 1 to 11 with a step size of 2:

```{r}
knn <- train(x = norm_train_data, y= train_label$cancer, method = "knn", 
             tuneGrid = data.frame(k = seq(1,11,2)))
knn
```

```{r, message=FALSE, fig.width=5, fig.height=3}
ggplot(knn, highlight = T) + labs(title="kNN", x="k")
```


Using the model to make predictions on the test data
```{r}

knn_prediction <- predict(knn, norm_test_data)
confusionMatrix(knn_prediction, factor(test_label$cancer))
```

Accuracy is the proportion of correct predictions

The kappa statistic adjusts accuracy
by accounting for the possibility of a correct prediction by chance alone. 

The “no-information rate” is the largest proportion of the observed classes:
```{r}
mean(knn_prediction == "ALL")
```
P-Value [Acc > NIR]: A hypothesis test computed to evaluate whether the overall accuracy rate is greater than the rate of the largest class

Sensitivity is the true positive rate (in this case ALL is the positive class)
sensitivity = $\frac{TP}{TP+FP}$

Specificity is the true negativ rate (in this case AML is the negative class)
sensitivity = $\frac{TN}{TN+FP}$

Pos Pred Value = $\frac{TP}{TP+FP}$

Neg Pred Value = $\frac{TN}{TN+FN}$

Saving the accuracy

```{r}
results <- tibble(model="knn", accuracy=confusionMatrix(knn_prediction,
                                                    factor(test_label$cancer))$overall[["Accuracy"]])
```




# Support Vector Machines
Support vector machines are supervised learning models for classification and regression analysis. They are effective in high dimensional spaces. So they should be useful for classification tasks with many features, such as the classification of microarray data. 
The goal of an SVM is to create a flat boundary called a hyperplane, which divides the space to create fairly homogeneous partitions
on either side. [6] The Hyperplane is chosen to create the greatest  separation between the two classes: maximum margin hyperplane.
A cost value can be applied to all points, which do not fall on the right side of the hyperplane. The greater the cost parameter, the harder the optimization will try to achieve 100 percent separation. On the other hand, a lower cost parameter will place the emphasis on a wider overall margin. Choosing the cost factor too large can lead to overfitting on the training data.
Many datasets which are not linearly divisible can still be analyzed by projecting them into a higher dimension, where they are separable. This is called the kernel trick.

There are several kernel functions available.

The linear kernel does not transform the data at all.

The radial kernel function for two points X₁ and X₂ computes the similarity or how close they are to each other.

The polynomial kernel of degree d adds a simple nonlinear transformation
of the data.

The sigmoid kernel results in an SVM model somewhat analogous to a neural
network using a sigmoid activation function

## Linear SVM

```{r}
svmlinear1 <- train(x = norm_train_data, y= train_label$cancer, method = "svmLinear")
svmlinear1
```

Using the model to make predictions on the test data

```{r}
svmlinear1_prediction <- predict(svmlinear1, norm_test_data)
confusionMatrix(svmlinear1_prediction, factor(test_label$cancer))
```

```{r}
svmlinear1_accuracy <- confusionMatrix(svmlinear1_prediction,
                                            factor(test_label$cancer))$overall[["Accuracy"]]
results <- rbind(results, c("SVM Linear ", svmlinear1_accuracy))

```

Without optimizing the cost factor we have already achieved an accuracy of `r svmlinear1_accuracy`

## Radial SVM
```{r}
svmRadial1 <- train(x = norm_train_data, y= train_label$cancer, method = "svmRadial", tuneLength = 10)
svmRadial1$results

svmRadial1_predictions <- predict(svmRadial1, norm_test_data)
confusionMatrix(svmRadial1_predictions, factor(test_label$cancer))

```

```{r}
svmRadial1_accuracy <- confusionMatrix(svmRadial1_predictions, factor(test_label$cancer))$overall[["Accuracy"]]
results <- rbind(results, c("SVM Radial", svmRadial1_accuracy))
```


 
# Discussion
This analysis has shown that the automated diagnosis of ALL versus AML is feasible with microarray expression data and SVM after differentially expressed genes have been identified.

```{r}
knitr::kable(results)
```

Linear SVM has achieved the highest accuracy.

The limitation of this project is the fact that only a relatively small test set was available. The high accuracy might to some degree be due to chance.
Also this project might be repeated with RNAseq data. RNAseq continues to replace microarray experiments, since it hypothesis free. With microarrays only known genes wich are represented on the chip can be detected.
                                                            
# References
[1] Demystifying Targeted Cancer Treatment. Course. Cancer Research UK
[2] Lesk, Arthur M. : Introduction to Genomics. Oxford 2017.
[3] Golub et. al.: Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring. Science 286, 531 (1999)
[4] Ritchie, ME, Phipson, B, Wu, D, Hu, Y, Law, CW, Shi, W, and Smyth, GK (2015).
limma powers differential expression analyses for RNA-sequencing and microarray studies.
Nucleic Acids Research 43(7), e47.
[5] https://bioconductor.org/packages/release/workflows/vignettes/RNAseq123/inst/doc/designmatrices.html
[6] Lantz, B: Machine Learning with R. 2019 Packt Publishing